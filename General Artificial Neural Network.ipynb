{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Artificial Neural Network (THE CONCEPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font color=\"red\" size=5>__Step 1 :__ </font><font size=4> Randomly initialize the weights to a small number close to 0 ( but not 0 ) </font>\n",
    "![downward_arrow](downward_arrow.png)\n",
    "<font color=\"red\" size=5>__Step 2 :__ </font><font size=4> Input the first observation of your dataset in the input layer, each feature in one input node </font>\n",
    "![downward_arrow](downward_arrow.png)\n",
    "<font color=\"red\" size=5>__Step 3 :__ </font><font size=4>__Forward Propagation:__ from left to right, the neurons are activated in a way that the impact of each neuron's activation is limited by weights. Propagate the activations until getting the predicted result y</font>\n",
    "![downward_arrow](downward_arrow.png)\n",
    "<font color=\"red\" size=5>__Step 4 :__ </font><font size=4>Compare the predicted result to the actual result. Measure the generated error</font>\n",
    "![downward_arrow](downward_arrow.png)\n",
    "<font color=\"red\" size=5>__Step 5 :__ </font><font size=4>__Back Propagation:__ from right to left. the error is back-propagated. Update the weights according to how much they are responsible for the error. The __Learning Rate__ decides how much to update the weights</font>\n",
    "![downward_arrow](downward_arrow.png)\n",
    "<font color=\"red\" size=5>__Step 6 :__ </font><font size=4>Repeat steps 1 to 5 and update the weights after each observation (__Stochastic Gradient Descent / Reinforcement Learning__), Or, Repeat steps 1 to 5 but update the weights only after a batch of observations (__Batch Gradient Descent / Batch Learning__)</font>\n",
    "![downward_arrow](downward_arrow.png)\n",
    "<font color=\"red\" size=5>__Step 7 :__ </font><font size=4>When the whole training set has been passed through the ANN, this completes 1 __Epoch__. Repeat for more epochs</font>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Working Code Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 18.0\n",
      "    Uninstalling pip-18.0:\n",
      "      Successfully uninstalled pip-18.0\n",
      "Successfully installed pip-19.1.1\n",
      "Requirement already satisfied: keras in c:\\users\\sharmo.sarkar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sharmo.sarkar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: keras-applications==1.0.4 in c:\\users\\sharmo.sarkar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.0.4)\n",
      "Requirement already satisfied: h5py in c:\\users\\sharmo.sarkar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\sharmo.sarkar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing==1.0.2 in c:\\users\\sharmo.sarkar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\sharmo.sarkar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.14.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\sharmo.sarkar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A Classification Problem (Churn Prediction)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape :  (10000, 10)\n",
      "y shape :  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Datasets\\Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "print('X Shape : ', X.shape)\n",
    "print('y shape : ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# ## label encoding Country\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "\n",
    "# ## label encoding Gender\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "\n",
    "# ## Since categorical variables are not ordinal, we need to convert them to the one-hot encoded feature \n",
    "# ## but we already have a kind of 0/1 encoding for Gender. Hence, we do this only for the Country\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "# ## Remove one of the dependent variables generated from dummy variables to avoid the Dummy Variable Trap \n",
    "# ## Ref: https://www.algosome.com/articles/dummy-variable-trap-regression.html\n",
    "# ## removing one of the dummy variables from the country's dummy variable\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Split the dataset to Test and Train Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## IMPOTANT !! Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras # keras with Tensorflow Backend\n",
    "from keras.models import Sequential # used to initialize the ANN\n",
    "from keras.layers import Dense # used to create the layers in the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "1. We have 11 independent variable, hence 11 input nodes are needed (Ref. Step 1 in Prev Section)\n",
    "2. The Neurons are Activated by the Activation Function from left to right in such a way that the higher the value of Activation function is for the neuron, the more impact the neuron is going to have on the entire network ie. more it will pass on the signal from the nodes in the left to the nodes in the right\n",
    "3. We will choose the Rectilinear Activation Function for hidden layer and Sigmoid Activation Function for the Output Layer, because using sigmoid we could also get the probabilities associated with our predictions\n",
    "\n",
    "TIPS:\n",
    "1. Experimentation has shown it's usually good to use __Number of Nodes in Hideen Layer = Avg(Number of nodes in Input Layer, Number of Nodes in Output Layer)__ OR use __Parameter Tuning methods__ to determine the optimal number of Nodes in the Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## I define it as sequence of Layers\n",
    "\n",
    "# ## Initializing the ANN \n",
    "classifier = Sequential()\n",
    "\n",
    "# ## Adding the Input Layer and the first Hidden Layer\n",
    "# ## we add a Dense layer as the first Hidden Layer \n",
    "# ## initialize the weights on the input(previous) layer with numbers close to 0, we use 'uniform' to draw it from a uniform distribution\n",
    "# ## number of nodes on hidden layer = (input_nodes + output_nodes)/2 = (11+1)/2 = 6 = output_dim\n",
    "# ## activation function on each of hidden layers is Rectifier Activation function\n",
    "# ## input dim = number of nodes in the input layer = number of independent variables (need to specify this for 1st hidden layer)\n",
    "classifier.add(layer=Dense(output_dim=6, kernel_initializer='uniform', activation='relu', input_dim=11)) \n",
    "\n",
    "# ## Adding a Second Hidden Layer\n",
    "# ## NOTE: input_dim is not specified because it knows what input dim to expect from the previous hidden layer\n",
    "classifier.add(layer=Dense(output_dim=6, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# ## Adding the Output Layer\n",
    "# ## output_dim = 1 (a Binary output is required)\n",
    "# ## We need the probabilities of our predictions --> use Sigmoid Activation Function\n",
    "# ## NOTE: if there are 3 categories(classes) in output, then output_dim=3 and activation='softmax'\n",
    "classifier.add(layer=Dense(output_dim=1, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# ## Compiling the ANN\n",
    "# ## optimizer -- Algo used to get the optimal set of weights in the Neural Network (Stochastic gradient descent). adam is a type of SGD\n",
    "# ## loss -- loss function within the specified SGD algo (this is a loss func that optizes the weights using the SGD)\n",
    "# ## NOTE: We are using Logarithmic loss function here\n",
    "# ## NOTE: If our output variable is Binary -- log loss func -- 'binary_crossentropy'\n",
    "# ## NOTE: If our output variable is more than 2 variables -- log loss func -- 'categorical_crossentropy'\n",
    "# ## matrix -- evaluation criterion used throughout back n forward propagation \n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the ANN to Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.3487 - acc: 0.8557\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3441 - acc: 0.8557\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 0.3448 - acc: 0.8545\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3488 - acc: 0.8537\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3501 - acc: 0.8526\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3437 - acc: 0.8524\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3486 - acc: 0.8549\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3419 - acc: 0.8540\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3488 - acc: 0.8525\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3422 - acc: 0.8542\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3440 - acc: 0.8529\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3437 - acc: 0.8546\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3462 - acc: 0.8532\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 0.3447 - acc: 0.8549\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.3527 - acc: 0.8536\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.3508 - acc: 0.8497\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3413 - acc: 0.8530\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3410 - acc: 0.8531\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3407 - acc: 0.8522\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3426 - acc: 0.8519\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3407 - acc: 0.8519\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3472 - acc: 0.8530\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 0.3447 - acc: 0.8504\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 0.3408 - acc: 0.8537\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 0.3467 - acc: 0.8531\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 0.3413 - acc: 0.8539\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3396 - acc: 0.8531\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 0.3425 - acc: 0.8536\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3427 - acc: 0.8562\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 0.3475 - acc: 0.8509\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3423 - acc: 0.8554\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3480 - acc: 0.8541\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 0.3463 - acc: 0.8505\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.3444 - acc: 0.8535\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3455 - acc: 0.8509\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 0.3565 - acc: 0.8389\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3454 - acc: 0.8494\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3400 - acc: 0.8526\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3430 - acc: 0.8551\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3492 - acc: 0.8532\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3429 - acc: 0.8536\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3397 - acc: 0.8516\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.3398 - acc: 0.8500\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 0.3416 - acc: 0.8516\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 0.3388 - acc: 0.8520\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 0.3457 - acc: 0.8514\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3558 - acc: 0.8467\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3556 - acc: 0.8424\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3471 - acc: 0.8501\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3492 - acc: 0.8510\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3434 - acc: 0.8521\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3397 - acc: 0.8537\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3395 - acc: 0.8545\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3442 - acc: 0.8495\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3400 - acc: 0.8539\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 0.3367 - acc: 0.8514\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3443 - acc: 0.8536\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3408 - acc: 0.8540\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 0.3389 - acc: 0.8530\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3372 - acc: 0.8510: 0s - loss: 0.3115 - a\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.3418 - acc: 0.8496\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3385 - acc: 0.8552\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3360 - acc: 0.8517\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3403 - acc: 0.8539\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3389 - acc: 0.8517\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3379 - acc: 0.8506\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3429 - acc: 0.8516\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3455 - acc: 0.8506\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3437 - acc: 0.8504\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.3395 - acc: 0.8495\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.3471 - acc: 0.8475\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 0.3409 - acc: 0.8507\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3416 - acc: 0.8511\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3390 - acc: 0.8529\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3412 - acc: 0.8519\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3417 - acc: 0.8536\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3354 - acc: 0.8522\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 0.3424 - acc: 0.8490\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 0.3406 - acc: 0.8491\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3397 - acc: 0.8522\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.3392 - acc: 0.8520\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3405 - acc: 0.8530\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3356 - acc: 0.8516\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3426 - acc: 0.8512\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3431 - acc: 0.8525\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3372 - acc: 0.8520\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3399 - acc: 0.8541\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3393 - acc: 0.8507\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3443 - acc: 0.8516\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3434 - acc: 0.8549\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3428 - acc: 0.8516\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3390 - acc: 0.8525\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3398 - acc: 0.8516\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3359 - acc: 0.8512\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3432 - acc: 0.8522\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3382 - acc: 0.8507: 0s - loss: 0.3110 - a\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.3436 - acc: 0.8497\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3435 - acc: 0.8501\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.3396 - acc: 0.8501\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.3390 - acc: 0.8527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c4b4192e80>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Fit the ANN\n",
    "# ## Batch size and Number of Epochs need to be derieved from Parameter Tuning ( but here we use some fixed values )\n",
    "classifier.fit(x=X_train, y=y_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1517,   78],\n",
       "       [ 190,  215]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Making Predictions on the Test Set ( Returns the Probabilities)\n",
    "y_pred = classifier.predict(X_test)\n",
    "# ## changing the probabilities to actualy 1/0 values\n",
    "y_pred = (y_pred > 0.5) # if the value of y_pred is < 0.5, then 0 otherwise 1\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "# *Notes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Point 1 :::\n",
    " ### A Simple Neural Network with just __1 Neuron__ like the architecture below is called a __Perceptron Model__\n",
    " <br/>\n",
    " ![perceptron](images/ANN_perceptron.png) \n",
    " <br/>\n",
    " ### Now if we use a __Sigmoid Activation Function__ for this Perceptron we get a __Logistic Regression__\n",
    " <br/>\n",
    " ![logistic_from_perceptron](images/ANN_logistic_from_perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 2 :::\n",
    " \n",
    "### Keras Regression Metrics\n",
    "  \n",
    " * __Mean Squared Error__: mean_squared_error, MSE or mse\n",
    " * __Mean Absolute Error__: mean_absolute_error, MAE, mae\n",
    " * __Mean Absolute Percentage Error__: mean_absolute_percentage_error, MAPE, mape\n",
    " * __Cosine Proximity__: cosine_proximity, cosine\n",
    "\n",
    "### Keras Classification Metrics\n",
    "  \n",
    " * __Binary Accuracy__: binary_accuracy, acc\n",
    " * __Categorical Accuracy__: categorical_accuracy, acc\n",
    " * __Sparse Categorical Accuracy__: sparse_categorical_accuracy\n",
    " * __Top k Categorical Accuracy__: top_k_categorical_accuracy (requires you specify a k parameter)\n",
    " * __Sparse Top k Categorical Accuracy__: sparse_top_k_categorical_accuracy (requires you specify a k parameter)\n",
    " \n",
    "#### Quick Read: [Keras Metrics] (https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
